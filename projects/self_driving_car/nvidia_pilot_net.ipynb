{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from config import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://arxiv.org/pdf/1704.07911.pdf\n",
    "# https://arxiv.org/pdf/1708.03798.pdf\n",
    "# data: https://github.com/udacity/self-driving-car/tree/master/datasets/CH2\n",
    "# scripts to convert rosbags to csv: https://github.com/rwightman/udacity-driving-reader\n",
    "\n",
    "# TODO: \n",
    "# 1. Input image need to be changed to YUV (img_yuv = cv2.cvtColor(image, cv2.COLOR_BGR2YUV))\n",
    "# 2. Cropping to 200 x 66 per paper\n",
    "# 3. Figure out proper steps for training per epoch.\n",
    "# 4. Split data into train and validation.\n",
    "# 5. Incorporate testing framework.\n",
    "# 6. Incorporate visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['frame_id', 'filename', 'angle']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_header = \"index,timestamp,width,height,frame_id,filename,angle,torque,speed,lat,long,alt\".split(\",\")\n",
    "csv_header[-8:-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "home = config['bag4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(file_path):    \n",
    "    def decode_csv(line):\n",
    "        # tf.decode_csv needs a record_default as 2nd parameter\n",
    "        data = tf.decode_csv(line,  list(np.array([\"\"]*12).reshape(12,1)))[-8:-5]\n",
    "        img_path = home+data[1]\n",
    "        img_decoded = tf.to_float(tf.image.decode_image(tf.read_file(img_path)))\n",
    "        \n",
    "        # normalize between (-1,1)\n",
    "        img_decoded = -1.0 + 2.0 * img_decoded / 255.0\n",
    "        steer_angle = tf.string_to_number(data[2], tf.float32)\n",
    "        \n",
    "        # return camera as well to confirm location of camera \n",
    "        # e.g left_camera, center_camera, right_camera...we want center_camera\n",
    "        return {'image':  img_decoded, 'camera': data[0]}, [steer_angle]\n",
    "\n",
    "    # skip() header row,\n",
    "    # filter() for center camera,\n",
    "    # map() transform each line by applying decode_csv()\n",
    "    dataset = (tf.data.TextLineDataset(file_path) \n",
    "        .skip(1) \n",
    "        .filter(lambda x: tf.equal(\n",
    "            tf.string_split(tf.reshape(x,(1,)),',').values[4],\n",
    "            'center_camera'))\n",
    "        .map(decode_csv)) \n",
    "               \n",
    "    return dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=home+\"interpolated.csv\"\n",
    "dataset = input_fn(file_path)\n",
    "dataset = dataset.batch(32) \n",
    "batch_generator = dataset.make_one_shot_iterator()\n",
    "batch = batch_generator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sample = sess.run(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['center_camera', 'center_camera', 'center_camera', 'center_camera',\n",
       "       'center_camera', 'center_camera', 'center_camera', 'center_camera',\n",
       "       'center_camera', 'center_camera', 'center_camera', 'center_camera',\n",
       "       'center_camera', 'center_camera', 'center_camera', 'center_camera',\n",
       "       'center_camera', 'center_camera', 'center_camera', 'center_camera',\n",
       "       'center_camera', 'center_camera', 'center_camera', 'center_camera',\n",
       "       'center_camera', 'center_camera', 'center_camera', 'center_camera',\n",
       "       'center_camera', 'center_camera', 'center_camera', 'center_camera'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm only center camera data returned.\n",
    "sample[0]['camera']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    " class PilotNet(object):\n",
    "    \n",
    "    def __init__(self, sess, log_dir, n_epochs, batch_size):\n",
    "        self._sess = sess\n",
    "        self._log_dir = log_dir\n",
    "        self._n_epochs = n_epochs\n",
    "        self._batch_size = batch_size\n",
    "        self._build_graph()\n",
    "\n",
    "    def _model(self, x):\n",
    "        assert(x[0].shape == (480,640,3))\n",
    "        out = tf.layers.batch_normalization(x)\n",
    "        out = tf.layers.conv2d(x, 24, [5,5], (2,2), \"valid\", activation=tf.nn.relu)\n",
    "        out = tf.layers.conv2d(out, 36, [5,5], (2,2), \"valid\", activation=tf.nn.relu)\n",
    "        out = tf.layers.conv2d(out, 48, [5,5], (2,2), \"valid\", activation=tf.nn.relu)\n",
    "        out = tf.layers.conv2d(out, 64, [3,3], (1,1), \"valid\", activation=tf.nn.relu)\n",
    "        out = tf.layers.conv2d(out, 64, [3,3], (1,1), \"valid\", activation=tf.nn.relu)\n",
    "        out = tf.reshape(out, [-1, 64*53*73])\n",
    "        out = tf.layers.dense(out, 100, tf.nn.relu) \n",
    "        out = tf.layers.dense(out, 50, tf.nn.relu) \n",
    "        out = tf.layers.dense(out, 10, tf.nn.relu) \n",
    "        out = tf.layers.dense(out, 1)\n",
    "        return out\n",
    "    \n",
    "    def _build_graph(self):\n",
    "        self._inputs = tf.placeholder(\"float\", [None, 480, 640, 3])\n",
    "        self._targets = tf.placeholder(\"float\", [None, 1])\n",
    "        self._global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "        self._predict = self._model(self._inputs)\n",
    "        self._loss = tf.losses.mean_squared_error(labels=self._targets, predictions=self._predict)\n",
    "        self._train =  tf.train.AdamOptimizer().minimize(self._loss, global_step=self._global_step)\n",
    "\n",
    "    def _train_admin_setup(self):\n",
    "        # Writers\n",
    "        self._train_writer = tf.summary.FileWriter(self._log_dir + '/train', self._sess.graph)\n",
    "\n",
    "        # Saver\n",
    "        self._saver = tf.train.Saver()\n",
    "\n",
    "        # Summaries\n",
    "        tf.summary.scalar(\"loss\", self._loss)\n",
    "        self._all_summaries = tf.summary.merge_all()\n",
    "        \n",
    "    def train(self, file_path):\n",
    "        self._train_admin_setup()\n",
    "        tf.global_variables_initializer().run()\n",
    "        \n",
    "        for epoch in range(self._n_epochs):\n",
    "            dataset = input_fn(file_path)\n",
    "            dataset = dataset.batch(self._batch_size)\n",
    "            batch_generator = dataset.make_one_shot_iterator()\n",
    "            epoch_loss = 0\n",
    "            for step in range(10):\n",
    "                img_batch, label_batch = self._sess.run(batch_generator.get_next())\n",
    "                \n",
    "                loss, _ = self._sess.run([self._loss, self._train], feed_dict={\n",
    "                    self._inputs: img_batch['image'], \n",
    "                    self._targets: label_batch}\n",
    "                )\n",
    "                \n",
    "                # 10 should be number of train samples / batch size\n",
    "                # Need to fix to iterate over all mini-batches.\n",
    "                epoch_loss += loss/10 \n",
    "\n",
    "            # Add summary and save checkpoint after every epoch\n",
    "            s = self._sess.run(self._all_summaries, feed_dict={\n",
    "                self._inputs : img_batch['image'],\n",
    "                self._targets: label_batch}\n",
    "            )\n",
    "            self._train_writer.add_summary(s, global_step=epoch)\n",
    "            self._saver.save(self._sess, 'checkpoints/pilot_net', global_step=self._global_step)\n",
    "            print(\"Epoch: {} Loss: {}\".format(epoch, epoch_loss))\n",
    "\n",
    "\n",
    "        # Need to closer writers\n",
    "        self._train_writer.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 1.33559389146\n",
      "Epoch: 1 Loss: 0.0316929148394\n",
      "Epoch: 2 Loss: 0.0189992283471\n",
      "Epoch: 3 Loss: 0.0415217437549\n",
      "Epoch: 4 Loss: 0.0133060722728\n"
     ]
    }
   ],
   "source": [
    "file_path=home+\"interpolated.csv\"\n",
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    model = PilotNet(sess, \"pilot_net\", 10 ,32)\n",
    "    model.train(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
