{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from util.util_word2vec_preprocess import process_data\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# References\n",
    "# http://web.stanford.edu/class/cs20si/lectures/notes_04.pdf\n",
    "# https://stackoverflow.com/questions/34870614/what-does-tf-nn-embedding-lookup-function-do\n",
    "# https://www.tensorflow.org/tutorials/word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Word2Vec(object):\n",
    "    \n",
    "    def __init__(self, sess, batch_size, vocab_size, embed_size, n_samples, epochs):\n",
    "        self.sess = sess\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.n_samples = n_samples\n",
    "        self.epochs = epochs\n",
    "        self._build_graph()\n",
    "    \n",
    "    def _build_graph(self):\n",
    "        # Scalars (index in vocab) are being fed in thus dimensions is size of batch.\n",
    "        self.center = tf.placeholder(tf.int32, shape=[self.batch_size])\n",
    "        self.target = tf.placeholder(tf.int32, shape=[self.batch_size,1])\n",
    "        \n",
    "        # Initialize uninformly, with dimensions vocab size by embed size.\n",
    "        self.embed_matrix = tf.Variable(tf.random_uniform([self.vocab_size, self.embed_size], -1, 1))\n",
    "        self.w = tf.Variable(tf.truncated_normal(\n",
    "                [self.vocab_size, self.embed_size], stddev=0.1/self.embed_size**0.5)\n",
    "        )\n",
    "        self.b = tf.Variable(tf.zeros([self.vocab_size]))\n",
    "        \n",
    "        # The function embedding_lookup intuitively means \"select rows given the row id\"\n",
    "        # https://stackoverflow.com/questions/34870614/what-does-tf-nn-embedding-lookup-function-do\n",
    "        # Get the rows in self.embed_matrix, \"id\"-ed by self.center\n",
    "        self.embed = tf.nn.embedding_lookup(self.embed_matrix, self.center)\n",
    "        \n",
    "        \n",
    "        # NSE handles the computational complexity associated with applying the\n",
    "        # softmax function to a large class,\n",
    "        # i.e vocab size in this case.\n",
    "        # For intuitiive explanation and resources.\n",
    "        # https://datascience.stackexchange.com/questions/13216/intuitive-explanation\\\n",
    "        #-of-noise-contrastive-estimation-nce-loss\n",
    "        self.loss = tf.reduce_mean(tf.nn.nce_loss(\n",
    "                weights=self.w,\n",
    "                biases=self.b,\n",
    "                labels=self.target,\n",
    "                inputs=self.embed,\n",
    "                num_sampled=self.n_samples,\n",
    "                num_classes=self.vocab_size)\n",
    "        )\n",
    "        \n",
    "        # We will just use default settings for hyperparameters defined:\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer\n",
    "        # Cant beat RMSprop + momentum :)\n",
    "        self.optimizer = tf.train.AdamOptimizer().minimize(self.loss)    \n",
    "        \n",
    "    def train(self, batch_gen):\n",
    "        # Need to always initialize variables\n",
    "        tf.global_variables_initializer().run()\n",
    "        ave_loss = 0\n",
    "        for i_epoch in xrange(self.epochs):\n",
    "            # Get batch\n",
    "            batch = batch_gen.next()\n",
    "            loss, _ = self.sess.run([self.loss, self.optimizer],\n",
    "                feed_dict={\n",
    "                    self.center: batch[0],\n",
    "                    self.target: batch[1]\n",
    "                }\n",
    "            )\n",
    "            ave_loss += loss\n",
    "            if (i_epoch + 1)%1000 == 0:\n",
    "                print(\"Average loss at epoch {}: {:5.1f}\".format(\n",
    "                        i_epoch + 1, ave_loss/(i_epoch + 1)))\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ready\n",
      "Average loss at epoch 1000: 238.7\n",
      "Average loss at epoch 2000: 207.1\n",
      "Average loss at epoch 3000: 185.8\n",
      "Average loss at epoch 4000: 169.1\n",
      "Average loss at epoch 5000: 155.6\n",
      "Average loss at epoch 6000: 145.0\n",
      "Average loss at epoch 7000: 135.9\n",
      "Average loss at epoch 8000: 128.1\n",
      "Average loss at epoch 9000: 120.9\n",
      "Average loss at epoch 10000: 114.6\n"
     ]
    }
   ],
   "source": [
    "# Set up parameters \n",
    "vocab_size = 50000\n",
    "batch_size = 128\n",
    "embed_size = 128\n",
    "skip_window = 1\n",
    "n_samples = 64\n",
    "epochs = 10000\n",
    "\n",
    "# process_data is defined in utils\n",
    "batch_gen = process_data(vocab_size, batch_size, skip_window)\n",
    "\n",
    "# Set up graph and train\n",
    "with tf.Session() as sess:\n",
    "    model = Word2Vec(sess, batch_size, vocab_size, embed_size, n_samples, epochs)\n",
    "    model.train(batch_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
