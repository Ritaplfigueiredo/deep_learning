{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "# https://github.com/danijar/sets/tree/master/sets/dataset\n",
    "import sets\n",
    "# if 'ImportError: cannot import name bayesflow' try $ pip install --upgrade dask\n",
    "# if 'AttributeError: 'module' object has no attribute 'Ocr'' try $ pip install set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n",
      "1.13.3\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective is to train an LSTM to recognize handwritten characters inputted as a fixed length sequence of characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The utility functions and datasets were sourced from:https://github.com/danijar/sets/tree/master/sets/dataset\n",
    "# IMPORTANT: You will need python3 to run. If you need python3 in your jupyter notebook, try the below.\n",
    "# $ python3 -m pip install ipykernel\n",
    "# $ python3 -m ipykernel install --user\n",
    "\n",
    "def read_dataset():\n",
    "    \"\"\"\n",
    "    Dataset of handwritten words collected by Rob Kassel at the MIT Spoken\n",
    "    Language Systems Group. Each example contains the normalized letters of the\n",
    "    word, padded to the maximum word length. Only contains lower case letter,\n",
    "    capitalized letters were removed.\n",
    "    From: http://ai.stanford.edu/~btaskar/ocr/\n",
    "    \"\"\"\n",
    "    dataset = sets.Ocr()\n",
    "    dataset = sets.OneHot(dataset.target, depth=2)(dataset, columns=['target'])\n",
    "    dataset['data'] = dataset.data.reshape(\n",
    "        dataset.data.shape[:-2] + (-1,)).astype(float)\n",
    "    train, test = sets.Split(0.66)(dataset)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quickly assess sample set and dimensions of inputs and targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAABiCAYAAABAmyhhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ndEnKhBO6JGXCCV2SMuGELkmZ+B8jJRIkMkqlfgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f11d4b32e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get train and test data.\n",
    "train, test = read_dataset()\n",
    "\n",
    "data = train.sample(1)\n",
    "img = data.data\n",
    "label = data.target\n",
    "\n",
    "plt.figure()\n",
    "for i in range(9):\n",
    "    loc = int(\"19\"+str(i+1))\n",
    "    plt.subplot(loc)\n",
    "    plt.imshow(img[0,i].reshape(16,8))\n",
    "\n",
    "# Note: I am only showing up to 9 when the sequence is really 14 elements long.\n",
    "# Note: The first letter in the word is removed for consistency on caps or no caps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dimension: (1, 14, 128)\n",
      "Label dimension: (1, 14, 27)\n"
     ]
    }
   ],
   "source": [
    "print(\"Input dimension: {}\".format(img.shape))\n",
    "print(\"Label dimension: {}\".format(label.shape))\n",
    "# Input is 14 steps long, and of size 128 which is flattened image of dimensions 16x8\n",
    "# There are 27 labels. Note: 26 characters in alphabet. Index 0 is used as indicator for blank\n",
    "# thus when no character, index 0 will have a value of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RNNModel(object):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 sess,\n",
    "                 n_units,\n",
    "                 n_layers,\n",
    "                 input_size,\n",
    "                 seq_len,\n",
    "                 n_classes,\n",
    "                 p_dropout,\n",
    "                 batch_size):\n",
    "        self._sess = sess             # tf session\n",
    "        self._n_units = n_units       # number of hidden units per layer in RNN\n",
    "        self._n_layers = n_layers     # number of layers in RNN\n",
    "        self._seq_len = seq_len       # length of input sequence (a.k.a time step)\n",
    "        self._n_classes = n_classes   # number of classes (in this case 27)\n",
    "        self._input_size = input_size # size of input image, (16x8 flattened to 128)\n",
    "        self._p_drop= p_dropout       # probability of dropout\n",
    "        self._batch_size = batch_size # batch size for each iteration\n",
    "        \n",
    "        # Defined when graph built\n",
    "        self._dropout = None\n",
    "        self._inputs = None\n",
    "        self._target = None \n",
    "        self._pred = None\n",
    "        self._error = None\n",
    "        # Deine graph\n",
    "        self._build_graph()\n",
    "        \n",
    "    def _model(self, w, b):\n",
    "        def __cell():\n",
    "            # https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/python/ops/rnn_cell_impl.py\n",
    "            # Tensorflow defaults to no peephole LSTM: http://www.bioinf.jku.at/publications/older/2604.pdf\n",
    "            rnn_cell = tf.contrib.rnn.LSTMCell(self._n_units)\n",
    "            return tf.contrib.rnn.DropoutWrapper(rnn_cell, output_keep_prob=self._dropout)\n",
    "        \n",
    "        cell = tf.contrib.rnn.MultiRNNCell([__cell() for _ in range(self._n_layers)])\n",
    "        # Out is of dimension [batch_size, max_time, cell_state_size]\n",
    "        out, state = tf.nn.dynamic_rnn(cell=cell, inputs=self._inputs, dtype=tf.float32)\n",
    "        out = tf.reshape(out, [-1, self._n_units])\n",
    "        # Fully connected passthrough to softmax.\n",
    "        pred = tf.nn.softmax(tf.matmul(out, w) + b)\n",
    "        return tf.reshape(pred, [-1, self._seq_len, self._n_classes])\n",
    "        \n",
    "    def _build_graph(self):\n",
    "        \"\"\" Define graph to run. \"\"\"\n",
    "        # Create placeholders\n",
    "        self._inputs = tf.placeholder(tf.float32, [None, self._seq_len, self._input_size], name=\"input\")\n",
    "        self._target = tf.placeholder(\"float\", [None,self._seq_len, self._n_classes], name=\"target\")\n",
    "        self._dropout = tf.placeholder(tf.float32)\n",
    "        \n",
    "        # Define variables for final fully connected layer.\n",
    "        w_ho = tf.Variable(tf.truncated_normal([self._n_units, self._n_classes], stddev=0.01), name=\"fc_w\")\n",
    "        b_o = tf.Variable(tf.constant(0.1, shape=[self._n_classes]), name=\"fc_b\")\n",
    "        \n",
    "        # Create model\n",
    "        self._pred = self._model(w_ho, b_o)\n",
    "        \n",
    "        # Define loss as negative log loss. We want the parameters that minimize this loss function, as we\n",
    "        # are optimizing to obtain the parameters that best explains the given observations.        \n",
    "        self._loss = tf.reduce_mean(-tf.reduce_sum(self._target * tf.log(self._pred), [1, 2]), name=\"loss_nll\")\n",
    "        self._optim = tf.train.RMSPropOptimizer(0.003).minimize(self._loss, name=\"rmsprop_optim\")\n",
    "\n",
    "        # Use in evaluating accuracy. Gets the index with largest probability found along dim=2.\n",
    "        # Returns the error rate.\n",
    "        neq = tf.not_equal(tf.argmax(self._target, 2),tf.argmax(self._pred, 2))\n",
    "        self._error = tf.reduce_mean(tf.cast(neq, tf.float32))\n",
    "        \n",
    "    def _evaluate(self, test_data):\n",
    "        \"\"\" Used in evaluating model with test data\n",
    "        args: test_data, test data definied in sets.\n",
    "        \"\"\"\n",
    "        test_data_feed = {\n",
    "            self._inputs: test_data.data,\n",
    "            self._target: test_data.target,\n",
    "            self._dropout: 1.0\n",
    "        }\n",
    "        return self._sess.run([self._error], test_data_feed)\n",
    "    \n",
    "    def train(self, train_data, test_data):\n",
    "        \"\"\" Call to train.\n",
    "        args: train_data, train object defined in sets.\n",
    "        args: test_data, test data defined in sets.\n",
    "        \"\"\"\n",
    "        self.writer = tf.summary.FileWriter(\"./sequence_label_logs\")\n",
    "        self.writer.add_graph(self._sess.graph)\n",
    "        \n",
    "        tf.global_variables_initializer().run()\n",
    "        for epoch in range(10):\n",
    "            for _ in range(100):\n",
    "                batch = train_data.sample(self._batch_size)\n",
    "                train_data_feed = {\n",
    "                    self._inputs: batch.data,\n",
    "                    self._target: batch.target,\n",
    "                    self._dropout: self._p_drop\n",
    "                }\n",
    "                train_loss, _ = self._sess.run(\n",
    "                    [self._loss, self._optim],\n",
    "                    train_data_feed\n",
    "                )\n",
    "\n",
    "            if epoch % 1 == 0:\n",
    "                error = self._evaluate(test_data)\n",
    "                print('Epoch {:2d} error {:3.1f}%'.format(epoch + 1, 100 * error[0]))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4538, 14, 128)\n",
      "Epoch  1 error 64.2%\n",
      "Epoch  2 error 57.2%\n",
      "Epoch  3 error 51.4%\n",
      "Epoch  4 error 48.0%\n",
      "Epoch  5 error 47.7%\n",
      "Epoch  6 error 44.1%\n",
      "Epoch  7 error 41.7%\n",
      "Epoch  8 error 39.0%\n",
      "Epoch  9 error 38.8%\n",
      "Epoch 10 error 37.5%\n"
     ]
    }
   ],
   "source": [
    "_, seq_len, input_size = train.data.shape\n",
    "print(train.data.shape)\n",
    "n_classes = train.target.shape[2]\n",
    "n_units=100\n",
    "n_layers=2\n",
    "p_dropout=0.3\n",
    "batch_size=10\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    model=RNNModel(sess, n_units, n_layers, input_size, seq_len, n_classes, p_dropout, batch_size)\n",
    "    model.train(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
